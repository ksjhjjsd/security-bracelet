{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9h6UsZxAfHT",
        "outputId": "9956c332-6ce8-4fd2-9f7e-ff5fd04b85c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ ÙƒÙ„ ÙØ¦Ø©:\n",
            "label\n",
            "normal    430\n",
            "danger    386\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      danger       0.94      0.96      0.95        78\n",
            "      normal       0.96      0.94      0.95        86\n",
            "\n",
            "    accuracy                           0.95       164\n",
            "   macro avg       0.95      0.95      0.95       164\n",
            "weighted avg       0.95      0.95      0.95       164\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Sensor Logger\n",
        "def load_sensor_file(path):\n",
        "    df_raw = pd.read_csv(path, encoding='utf-16', header=None)\n",
        "    # Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø±Ø§Ø¨Ø¹ ÙÙŠÙ‡ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "    header = df_raw.iloc[3, 0].split('\\t')\n",
        "    # Ù…Ù† Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø®Ø§Ù…Ø³ ÙˆÙÙˆÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    data = df_raw.iloc[4:, 0].str.split('\\t', expand=True)\n",
        "    data.columns = header\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…\n",
        "    for c in data.columns:\n",
        "        data[c] = pd.to_numeric(data[c], errors='coerce')\n",
        "    data = data.dropna().reset_index(drop=True)\n",
        "    return data\n",
        "\n",
        "# Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ„ Ù…Ù„Ù\n",
        "stop_data  = load_sensor_file(\"stop.csv\")\n",
        "walk_data  = load_sensor_file(\"walk.csv\")\n",
        "shake_data = load_sensor_file(\"Shake.csv\")\n",
        "fall_data  = load_sensor_file(\"fall.csv\")\n",
        "\n",
        "# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù„ÙŠØ¨Ù„Ø§Øª (normal / danger)\n",
        "stop_data[\"label\"]  = \"normal\"\n",
        "walk_data[\"label\"]  = \"normal\"\n",
        "shake_data[\"label\"] = \"danger\"\n",
        "fall_data[\"label\"]  = \"danger\"\n",
        "\n",
        "# Ø¯Ù…Ø¬ Ø§Ù„ÙƒÙ„\n",
        "data_all = pd.concat([stop_data, walk_data, shake_data, fall_data],\n",
        "                     ignore_index=True)\n",
        "\n",
        "print(\"Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ ÙƒÙ„ ÙØ¦Ø©:\")\n",
        "print(data_all[\"label\"].value_counts())\n",
        "\n",
        "# Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù„ÙŠ Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙƒØ®ØµØ§Ø¦Øµ\n",
        "feature_cols = [\"X\", \"Y\", \"Z\", \"|V|\", \"MAx\", \"MAy\", \"MAz\", \"MAv\"]\n",
        "X = data_all[feature_cols]\n",
        "y = data_all[\"label\"]\n",
        "\n",
        "# ØªÙ‚Ø³ÙŠÙ… Train / Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# ØªÙ‚ÙŠÙŠÙ…\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"\\nØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ¬Ø±Ø¨Ø© Ø¹Ù„Ù‰ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯\n",
        "def predict_file(path):\n",
        "    df_new = load_sensor_file(path)\n",
        "    X_new = df_new[feature_cols]\n",
        "    preds = rf.predict(X_new)\n",
        "    counts = Counter(preds)\n",
        "    print(\"ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª:\", counts)\n",
        "    if counts[\"danger\"] > counts[\"normal\"]:\n",
        "        print(\"ðŸ”´ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: DANGER (Ø­Ø±ÙƒØ© Ø®Ø·Ø±Ø©)\")\n",
        "    else:\n",
        "        print(\"ðŸŸ¢ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: NORMAL (Ø­Ø±ÙƒØ© Ø·Ø¨ÙŠØ¹ÙŠØ©)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_file(\"new.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_mINfVJBRGA",
        "outputId": "e94730fe-0146-4fb2-9b9a-da7e366b0bed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª: Counter({'danger': 172, 'normal': 5})\n",
            "ðŸ”´ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: DANGER (Ø­Ø±ÙƒØ© Ø®Ø·Ø±Ø©)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_file(\"wq.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLsNuKh0BTH9",
        "outputId": "fc7af627-5b85-4e4f-cb93-c0c168129253"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª: Counter({'normal': 195, 'danger': 79})\n",
            "ðŸŸ¢ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: NORMAL (Ø­Ø±ÙƒØ© Ø·Ø¨ÙŠØ¹ÙŠØ©)\n"
          ]
        }
      ]
    }
  ]
}